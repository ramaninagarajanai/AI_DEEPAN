{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ef6f457",
   "metadata": {},
   "source": [
    "Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cb00e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47c30ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab4bf67",
   "metadata": {},
   "source": [
    "Connect to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c892c928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(openai_api_base = \"http://localhost:1234/v1\", openai_api_key = \"lm_studio\", model = \"qwen3-0.6b\",temperature=0.8,model_kwargs={\"seed\": 42}) #max_tokens=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "912dec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(openai_api_base = \"https://openrouter.ai/api/v1\", openai_api_key = \"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\", model = \"deepseek/deepseek-chat-v3.1:free\",temperature=0.9) #max_tokens=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e0d1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.llms import Ollama\n",
    "# llm = Ollama(model=\"llama3.2:1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35faadf7",
   "metadata": {},
   "source": [
    "Remove thinking Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b285f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_think_sections(text):\n",
    "    return re.sub(r\"<think>.*?</think>\", \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86516d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"What would be a good company name for a company that makes colorful socks?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0b4dee",
   "metadata": {},
   "source": [
    "Basic Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e2a7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm.predict(text)\n",
    "output = remove_think_sections(output)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb43815",
   "metadata": {},
   "source": [
    "Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b8528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['cuisine'],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "formatted_prompt = prompt.format(cuisine=\"Chinese\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17fe5d4",
   "metadata": {},
   "source": [
    "Few Shot Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3cb52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\"word\": \"cat\", \"definition\": \"A small domesticated carnivorous mammal.\"},\n",
    "    {\"word\": \"dog\", \"definition\": \"A domesticated carnivorous mammal that typically has a long snout.\"},\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"word\", \"definition\"],\n",
    "    template=\"Word: {word}\\nDefinition: {definition}\\n\"\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Keep the answer very short and consise and Define the following words:\",\n",
    "    suffix=\"Word: {input}\\nDefinition:\",\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "\n",
    "output = (few_shot_prompt.format(input=\"elephant\"))\n",
    "print (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a42b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm.predict(output)\n",
    "output = remove_think_sections(output)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9875af3f",
   "metadata": {},
   "source": [
    "Simple Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d87ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Give me a tweet idea about {topic}\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "response = chain.run(\"AI in education\")\n",
    "response = remove_think_sections(response)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54f0d45",
   "metadata": {},
   "source": [
    "Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d55046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "# Prompt 1: Generate title\n",
    "title_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Write a compelling blog post title about {topic}. only give one one title as the output nothing more than that\"\n",
    ")\n",
    "title_chain = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")\n",
    "\n",
    "# Prompt 2: Use title to generate intro\n",
    "intro_prompt = PromptTemplate(\n",
    "    input_variables=[\"title\"],\n",
    "    template=\"Write a short blog introduction for the post titled: {title}. only give the introduction as the output nothing more than that. have bullet points and emojis in the intro\"\n",
    ")\n",
    "intro_chain = LLMChain(llm=llm, prompt=intro_prompt, output_key=\"intro\")\n",
    "\n",
    "# Chain them together\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[title_chain, intro_chain],\n",
    "    input_variables=[\"topic\"],\n",
    "    output_variables=[\"title\", \"intro\"]\n",
    ")\n",
    "\n",
    "result = overall_chain.invoke(\"LangChain for AI-powered apps\")\n",
    "# result = remove_think_sections(result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74833574",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[\"intro\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2ee2c7",
   "metadata": {},
   "source": [
    "Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db39874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(openai_api_base = \"http://localhost:1234/v1\", openai_api_key = \"lm_studio\", model = \"qwen3-0.6b\",temperature=0.9) #max_tokens=100\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True,  # shows the input/output/logs\n",
    ")\n",
    "\n",
    "print (conversation.predict(input=\"Hi, my name is Deepan.\"))\n",
    "\n",
    "\n",
    "print (conversation.predict(input=\"What’s my name?\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02088c1",
   "metadata": {},
   "source": [
    "Custom Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04332596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(openai_api_base = \"http://localhost:1234/v1\", openai_api_key = \"lm_studio\", model = \"qwen3-0.6b\",temperature=0.9) #max_tokens=100\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "class RemoveThinkParser(BaseOutputParser):\n",
    "    def parse(self, text: str) -> str:\n",
    "        # remove <think>...</think> blocks\n",
    "        return re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "    \n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    output_parser=RemoveThinkParser()  \n",
    ")\n",
    "\n",
    "print (conversation.predict(input=\"Hi, my name is Deepan.\"))\n",
    "\n",
    "\n",
    "print (conversation.predict(input=\"What’s my name?\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea138374",
   "metadata": {},
   "source": [
    "Custom Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb86e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "class RemoveThinkParser(BaseOutputParser):\n",
    "    def parse(self, text: str) -> str:\n",
    "        # remove <think>...</think> blocks\n",
    "        return re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "    \n",
    "custom_prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"],\n",
    "    template=\"\"\"\n",
    "You are a helpful and concise assistant.\n",
    "\n",
    "{history}\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    ")\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    prompt=custom_prompt,\n",
    "    output_parser=RemoveThinkParser() \n",
    ")\n",
    "\n",
    "\n",
    "print (conversation.predict(input=\"Hi, my name is Deepan.\"))\n",
    "\n",
    "\n",
    "print (conversation.predict(input=\"What’s my name?\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab91639",
   "metadata": {},
   "source": [
    "Conversation Summary Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac278a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "# llm = ChatOpenAI(openai_api_base = \"http://localhost:1234/v1\", openai_api_key = \"lm_studio\", model = \"llama-3.2-1b-instruct\",temperature=0.9) #max_tokens=100\n",
    "\n",
    "\n",
    "summary_memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "class RemoveThinkParser(BaseOutputParser):\n",
    "    def parse(self, text: str) -> str:\n",
    "        # remove <think>...</think> blocks\n",
    "        return re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "    \n",
    "custom_prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"],\n",
    "    template=\"\"\"\n",
    "You are a helpful and concise assistant.\n",
    "\n",
    "{history}\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    ")\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=summary_memory,\n",
    "    verbose=False,\n",
    "    prompt=custom_prompt,\n",
    "    output_parser=RemoveThinkParser() \n",
    ")\n",
    "\n",
    "\n",
    "print (conversation.predict(input=\"Hi, my name is Deepan.\"))\n",
    "\n",
    "print (conversation.predict(input=\"i want to learn about AI and Machine learning.\"))\n",
    "\n",
    "print (conversation.predict(input=\"how can you help me?\"))\n",
    "\n",
    "print (conversation.predict(input=\"What’s my name?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f25d8d",
   "metadata": {},
   "source": [
    "Memory with Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e4c580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template=\"The user said: {input}\"\n",
    ")\n",
    "\n",
    "chain_with_memory = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "chain_with_memory.predict(input=\"Hello there!\")\n",
    "chain_with_memory.predict(input=\"What did I just say?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b238887b",
   "metadata": {},
   "source": [
    "Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d42278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6512e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dad9c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "\n",
    "# set serpapi key\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"42a63592c032fc7a97f972c068c18c2055f7409b1886d53270e4e77d089fd71f\"\n",
    "\n",
    "# # local model\n",
    "# llm = ChatOpenAI(\n",
    "#     openai_api_base=\"http://localhost:1234/v1\",\n",
    "#     openai_api_key=\"lm_studio\",\n",
    "#     model=\"llama-3.2-1b-instruct\",\n",
    "#     temperature=0.9\n",
    "# )\n",
    "\n",
    "# tools\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm) \n",
    "\n",
    "# agent\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, \n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "# test\n",
    "print(agent.run(\"what the bitcoin price today and convert it into INR?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cd7e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1d66a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     openai_api_base=\"http://localhost:1234/v1\",\n",
    "#     openai_api_key=\"lm_studio\",\n",
    "#     model=\"llama-3.2-1b-instruct\",\n",
    "#     temperature=0.9\n",
    "# )\n",
    "\n",
    "# tools (wikipedia + math)\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,  \n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True  \n",
    ")\n",
    "\n",
    "\n",
    "print(agent.run(\"What is this year raised to the 0.43 power?\"\n",
    "                ))\n",
    "\n",
    "#In what year was the film Departed with Leonardo DiCaprio released?\n",
    "# \"What is this year raised to the 0.43 power?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d65c5c",
   "metadata": {},
   "source": [
    "Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1014d3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "class MyCustomHandler(BaseCallbackHandler):\n",
    "    def on_llm_start(self, serialized, prompts,**kwargs):\n",
    "        print(f\"LLM is starting with prompt: {prompts}\")\n",
    "\n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        print(f\"LLM finished. Response: {response}\")\n",
    "\n",
    "    def on_llm_error(self, error, **kwargs):\n",
    "        print(f\"LLM errored: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8d2bda28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is starting with prompt: ['Human: Hello!']\n",
      "LLM finished. Response: generations=[[ChatGeneration(text='How can I assist you today?', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 12, 'total_tokens': 19, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'llama-3.2-1b-instruct', 'system_fingerprint': 'llama-3.2-1b-instruct', 'finish_reason': 'stop', 'logprobs': None}, id='run-e8ec6aaa-df57-454e-bbd6-a151d58084f4-0'))]] llm_output={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 12, 'total_tokens': 19, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'llama-3.2-1b-instruct', 'system_fingerprint': 'llama-3.2-1b-instruct'} run=None\n",
      "How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_base=\"http://localhost:1234/v1\",\n",
    "    openai_api_key=\"lm_studio\",\n",
    "    model=\"llama-3.2-1b-instruct\",\n",
    "    callbacks=[MyCustomHandler()]\n",
    ")\n",
    "\n",
    "print(llm.predict(\"Hello!\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
