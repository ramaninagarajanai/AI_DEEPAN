{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoS7rOoL5q6H"
      },
      "outputs": [],
      "source": [
        "!pip install bert-score rouge_score\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score as bert_score\n",
        "import numpy as np\n",
        "from nltk.corpus import brown\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "yrrjodf88Qr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load and clean data\n",
        "sentences = brown.sents()\n",
        "sentences = [' '.join(sent).lower() for sent in sentences if len(sent) >= 3]\n",
        "sentences = sentences[:5000]  # Limit for quick training\n"
      ],
      "metadata": {
        "id": "HdOwA8Gf8TED"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Tokenize\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1"
      ],
      "metadata": {
        "id": "iKqEQb868Uol"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Create sequences with 4-word context and 1 target word\n",
        "X, y = [], []\n",
        "for sent in sentences:\n",
        "    tokens = tokenizer.texts_to_sequences([sent])[0]\n",
        "    for i in range(4, len(tokens)):\n",
        "        context = tokens[i-4:i]\n",
        "        target = tokens[i]\n",
        "        X.append(context)\n",
        "        y.append(target)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ],
      "metadata": {
        "id": "S3LV11qX8WXG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "embedding_dim = 100\n",
        "gru_units = 128\n"
      ],
      "metadata": {
        "id": "MuZ-ThwF8X4F"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Prepare the model\n",
        "model = Sequential()\n",
        "# Changed here: input_shape instead of input_length\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_shape=(2,)))\n",
        "model.add(GRU(gru_units))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "a6qQmjK98Zj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Train\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=256, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "LBpImiKI8iA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Generate text function\n",
        "def generate_text(seed_text, next_words=20):\n",
        "    result = seed_text.split()\n",
        "    for _ in range(next_words):\n",
        "        # Prepare input sequence\n",
        "        token_list = tokenizer.texts_to_sequences([result[-4:]])[0]\n",
        "        if len(token_list) < 4:\n",
        "            # Pad if less than 4 tokens (at generation start)\n",
        "            token_list = [0]*(4 - len(token_list)) + token_list\n",
        "        token_list = np.array(token_list).reshape(1, 4)\n",
        "\n",
        "        # Predict next word\n",
        "        predicted_probs = model.predict(token_list, verbose=0)[0]\n",
        "        predicted_index = np.argmax(predicted_probs)\n",
        "\n",
        "        # Map index to word\n",
        "        predicted_word = tokenizer.index_word.get(predicted_index, '')\n",
        "        if predicted_word == '':\n",
        "            break\n",
        "\n",
        "        result.append(predicted_word)\n",
        "    return ' '.join(result)"
      ],
      "metadata": {
        "id": "ZQh-5pfaATXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Evaluate on test set using BLEU, ROUGE (1,2,L), and BERTScore\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "bleu_scores = []\n",
        "rouge1_scores = []\n",
        "rouge2_scores = []\n",
        "rougeL_scores = []\n",
        "\n",
        "pred_texts = []\n",
        "true_texts = []\n",
        "\n",
        "num_eval_samples = 100  # limit eval for speed\n",
        "\n",
        "for i in range(min(num_eval_samples, len(X_test))):\n",
        "    context_tokens = X_test[i]\n",
        "    true_word_idx = y_test[i]\n",
        "\n",
        "    seed_words = [tokenizer.index_word.get(idx, '') for idx in context_tokens if idx != 0]\n",
        "    seed_text = ' '.join(seed_words)\n",
        "\n",
        "    input_seq = np.array(context_tokens).reshape(1, 4)\n",
        "    pred_probs = model.predict(input_seq, verbose=0)[0]\n",
        "    pred_idx = np.argmax(pred_probs)\n",
        "    pred_word = tokenizer.index_word.get(pred_idx, '')\n",
        "\n",
        "    true_word = tokenizer.index_word.get(true_word_idx, '')\n",
        "\n",
        "    # BLEU (1-gram)\n",
        "    bleu = sentence_bleu([true_word.split()], pred_word.split(), weights=(1, 0, 0, 0))\n",
        "    bleu_scores.append(bleu)\n",
        "\n",
        "    # ROUGE\n",
        "    rouge_scores = scorer.score(true_word, pred_word)\n",
        "    rouge1_scores.append(rouge_scores['rouge1'].fmeasure)\n",
        "    rouge2_scores.append(rouge_scores['rouge2'].fmeasure)\n",
        "    rougeL_scores.append(rouge_scores['rougeL'].fmeasure)\n",
        "\n",
        "    pred_texts.append(pred_word)\n",
        "    true_texts.append(true_word)\n",
        "\n",
        "print(f\"Avg BLEU score (next word): {np.mean(bleu_scores):.4f}\")\n",
        "print(f\"Avg ROUGE-1 F1: {np.mean(rouge1_scores):.4f}\")\n",
        "print(f\"Avg ROUGE-2 F1: {np.mean(rouge2_scores):.4f}\")\n",
        "print(f\"Avg ROUGE-L F1: {np.mean(rougeL_scores):.4f}\")\n",
        "\n",
        "# BERTScore (for single-word prediction, still illustrative)\n",
        "P, R, F1 = bert_score(pred_texts, true_texts, lang='en', verbose=True)\n",
        "print(f\"Avg BERTScore F1: {F1.mean():.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "CmFAj9eF5v2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Example generation\n",
        "\n",
        "print(\"\\nExample generated text:\")\n",
        "seed = \"I am a good\"\n",
        "generated = generate_text(seed, next_words=15)\n",
        "print(generated)\n"
      ],
      "metadata": {
        "id": "N8HNOmq4AZ9w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}