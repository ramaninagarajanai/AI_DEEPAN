{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets --quiet"
      ],
      "metadata": {
        "id": "DjXAL4cBXkK7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Install and Import Necessary Libraries ---\n",
        "import tensorflow as tf\n",
        "from transformers import TFDistilBertForQuestionAnswering, DistilBertTokenizer\n",
        "from datasets import load_dataset\n"
      ],
      "metadata": {
        "id": "dZ59Cj4IXl8W"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 2: Load the Dataset and Model ---\n",
        "print(\"Loading SQuAD dataset...\")\n",
        "# SQuAD (Stanford Question Answering Dataset) is the standard for this task.\n",
        "\n",
        "raw_datasets = load_dataset(\"squad\", split={\n",
        "    \"train\": \"train[:1%]\",      # Use 1% of the training data\n",
        "    \"validation\": \"validation[:5%]\" # Use 5% of the validation data\n",
        "})\n",
        "\n",
        "print(\"Loading DistilBERT model and tokenizer...\")\n",
        "# DistilBERT is a smaller, faster version of BERT.\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "\n",
        "from transformers import DistilBertTokenizerFast\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
        "model = TFDistilBertForQuestionAnswering.from_pretrained(model_name, use_safetensors=False)"
      ],
      "metadata": {
        "id": "xptTpF7MdQfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# --- Step 3: Preprocess the Data ---\n",
        "# This is the most complex step for QA. We need to tokenize the context and question\n",
        "# together and then map the answer text to the token positions.\n",
        "max_length = 384 # The maximum length of a feature (question and context)\n",
        "doc_stride = 128 # The authorized overlap between two parts of the context when splitting\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # Tokenize the questions and contexts, allowing for truncation and overlap.\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\"],\n",
        "        examples[\"context\"],\n",
        "        truncation=\"only_second\", # Truncate the context, not the question\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # The 'overflow_to_sample_mapping' maps each new feature back to its original example.\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "    # 'offset_mapping' maps each token to its character position in the original text.\n",
        "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "\n",
        "    # Now we label our data with the start and end token positions.\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "        # Grab the original example corresponding to this feature.\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = examples[\"answers\"][sample_index]\n",
        "\n",
        "        # If no answers are given, set the cls_index as the answer.\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "        else:\n",
        "            # Start and end character index of the answer in the text.\n",
        "            start_char = answers[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "            # Find the start and end token indices.\n",
        "            token_start_index = 0\n",
        "            while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                token_start_index += 1\n",
        "\n",
        "            token_end_index = len(offsets) - 1\n",
        "            while token_end_index >= 0 and offsets[token_end_index][1] >= end_char:\n",
        "                token_end_index -= 1\n",
        "\n",
        "            # If the answer is not fully inside the context, label it with cls_index.\n",
        "            if not (token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                 tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "                 tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "            else:\n",
        "                 tokenized_examples[\"start_positions\"].append(token_start_index - 1) # Adjust for python slicing\n",
        "                 tokenized_examples[\"end_positions\"].append(token_end_index - 1)\n",
        "\n",
        "\n",
        "    return tokenized_examples\n",
        "\n",
        "print(\"Tokenizing the dataset...\")\n",
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True, remove_columns=raw_datasets[\"train\"].column_names)"
      ],
      "metadata": {
        "id": "EHK5qZkLdedq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 4: Prepare Data for Training with tf.data ---\n",
        "train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"start_positions\", \"end_positions\"],\n",
        "    shuffle=True,\n",
        "    batch_size=8\n",
        ")\n",
        "\n",
        "validation_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"start_positions\", \"end_positions\"],\n",
        "    shuffle=False,\n",
        "    batch_size=8\n",
        ")"
      ],
      "metadata": {
        "id": "WMr5xh8bdh2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 5: Compile and Fine-Tune the Model ---\n",
        "print(\"Compiling the model...\")\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "# The model will compute the loss internally from start and end logits.\n",
        "model.compile(optimizer=optimizer)\n",
        "\n",
        "print(\"Starting fine-tuning...\")\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=3\n",
        ")"
      ],
      "metadata": {
        "id": "l2kWL32YdkTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXbnQIDIVX_j"
      },
      "outputs": [],
      "source": [
        "# --- Step 6: Inference\n",
        "import numpy as np\n",
        "\n",
        "def ask_question(model, tokenizer, question, context):\n",
        "    \"\"\"Asks a question to the fine-tuned model based on a context.\"\"\"\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(question, context, return_tensors=\"tf\")\n",
        "\n",
        "    # Get model outputs\n",
        "    outputs = model(inputs)\n",
        "    start_logits = outputs.start_logits\n",
        "    end_logits = outputs.end_logits\n",
        "\n",
        "    # Get the most likely start and end token positions\n",
        "    start_index = tf.argmax(start_logits, axis=1).numpy()[0]\n",
        "    end_index = tf.argmax(end_logits, axis=1).numpy()[0]\n",
        "\n",
        "    # Decode the tokens between start and end to get the answer\n",
        "    input_ids = inputs[\"input_ids\"].numpy()[0]\n",
        "    answer_tokens = input_ids[start_index : end_index + 1]\n",
        "    answer = tokenizer.decode(answer_tokens)\n",
        "\n",
        "    return answer\n",
        "\n",
        "# Example usage with a new context and question\n",
        "new_context = \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower.\"\n",
        "new_question = \"Who is the Eiffel Tower named after?\"\n",
        "\n",
        "print(\"\\n--- Answering a New Question ---\")\n",
        "print(f\"Context: {new_context}\")\n",
        "print(f\"Question: {new_question}\")\n",
        "answer = ask_question(model, tokenizer, new_question, new_context)\n",
        "print(f\"Predicted Answer: {answer}\")"
      ]
    }
  ]
}